{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "020b2f36",
   "metadata": {},
   "source": [
    "## Spambase assignment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4eb52c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Python Libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c11eaed",
   "metadata": {},
   "source": [
    "\n",
    "## Question 1.Import the spam dataset and print the first six rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3cc6eba3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_freq_make:</th>\n",
       "      <th>word_freq_address:</th>\n",
       "      <th>word_freq_all:</th>\n",
       "      <th>word_freq_3d:</th>\n",
       "      <th>word_freq_our:</th>\n",
       "      <th>word_freq_over:</th>\n",
       "      <th>word_freq_remove:</th>\n",
       "      <th>word_freq_internet:</th>\n",
       "      <th>word_freq_order:</th>\n",
       "      <th>word_freq_mail:</th>\n",
       "      <th>...</th>\n",
       "      <th>char_freq_;:</th>\n",
       "      <th>char_freq_(:</th>\n",
       "      <th>char_freq_[:</th>\n",
       "      <th>char_freq_!:</th>\n",
       "      <th>char_freq_$:</th>\n",
       "      <th>char_freq_#:</th>\n",
       "      <th>capital_run_length_average:</th>\n",
       "      <th>capital_run_length_longest:</th>\n",
       "      <th>capital_run_length_total:</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.223</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>15</td>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows Ã— 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_freq_make:  word_freq_address:  word_freq_all:  word_freq_3d:  \\\n",
       "0             0.00                0.64            0.64            0.0   \n",
       "1             0.21                0.28            0.50            0.0   \n",
       "2             0.06                0.00            0.71            0.0   \n",
       "3             0.00                0.00            0.00            0.0   \n",
       "4             0.00                0.00            0.00            0.0   \n",
       "5             0.00                0.00            0.00            0.0   \n",
       "\n",
       "   word_freq_our:  word_freq_over:  word_freq_remove:  word_freq_internet:  \\\n",
       "0            0.32             0.00               0.00                 0.00   \n",
       "1            0.14             0.28               0.21                 0.07   \n",
       "2            1.23             0.19               0.19                 0.12   \n",
       "3            0.63             0.00               0.31                 0.63   \n",
       "4            0.63             0.00               0.31                 0.63   \n",
       "5            1.85             0.00               0.00                 1.85   \n",
       "\n",
       "   word_freq_order:  word_freq_mail:  ...  char_freq_;:  char_freq_(:  \\\n",
       "0              0.00             0.00  ...          0.00         0.000   \n",
       "1              0.00             0.94  ...          0.00         0.132   \n",
       "2              0.64             0.25  ...          0.01         0.143   \n",
       "3              0.31             0.63  ...          0.00         0.137   \n",
       "4              0.31             0.63  ...          0.00         0.135   \n",
       "5              0.00             0.00  ...          0.00         0.223   \n",
       "\n",
       "   char_freq_[:  char_freq_!:  char_freq_$:  char_freq_#:  \\\n",
       "0           0.0         0.778         0.000         0.000   \n",
       "1           0.0         0.372         0.180         0.048   \n",
       "2           0.0         0.276         0.184         0.010   \n",
       "3           0.0         0.137         0.000         0.000   \n",
       "4           0.0         0.135         0.000         0.000   \n",
       "5           0.0         0.000         0.000         0.000   \n",
       "\n",
       "   capital_run_length_average:  capital_run_length_longest:  \\\n",
       "0                        3.756                           61   \n",
       "1                        5.114                          101   \n",
       "2                        9.821                          485   \n",
       "3                        3.537                           40   \n",
       "4                        3.537                           40   \n",
       "5                        3.000                           15   \n",
       "\n",
       "   capital_run_length_total:  spam  \n",
       "0                        278     1  \n",
       "1                       1028     1  \n",
       "2                       2259     1  \n",
       "3                        191     1  \n",
       "4                        191     1  \n",
       "5                         54     1  \n",
       "\n",
       "[6 rows x 58 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"C:/Python/assignment/spam_dataset.csv\")\n",
    "\n",
    "data.head(6)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2861742c",
   "metadata": {},
   "source": [
    "### Question 2. Read through the documentation of the original dataset here: http://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.names The dependent variable is \"spam\" where one indicates that an email is spam and zero otherwise. Which three variables in the dataset do you think will be important predictors in a model of spam? Why? \n",
    "\n",
    "Answer - word_freq_remove:, word_freq_your:, word_freq_000: -  This are the variables with the highest value of correlation to spam which is the target variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5400b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = data[['word_freq_remove:', 'word_freq_your:', 'word_freq_000:','spam']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "26090099",
   "metadata": {},
   "outputs": [],
   "source": [
    "#update data to set up for train test split\n",
    "X = final_data.loc[:, final_data.columns != 'spam']\n",
    "y = final_data['spam']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e1781c88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_freq_make:</th>\n",
       "      <th>word_freq_address:</th>\n",
       "      <th>word_freq_all:</th>\n",
       "      <th>word_freq_3d:</th>\n",
       "      <th>word_freq_our:</th>\n",
       "      <th>word_freq_over:</th>\n",
       "      <th>word_freq_remove:</th>\n",
       "      <th>word_freq_internet:</th>\n",
       "      <th>word_freq_order:</th>\n",
       "      <th>word_freq_mail:</th>\n",
       "      <th>...</th>\n",
       "      <th>char_freq_;:</th>\n",
       "      <th>char_freq_(:</th>\n",
       "      <th>char_freq_[:</th>\n",
       "      <th>char_freq_!:</th>\n",
       "      <th>char_freq_$:</th>\n",
       "      <th>char_freq_#:</th>\n",
       "      <th>capital_run_length_average:</th>\n",
       "      <th>capital_run_length_longest:</th>\n",
       "      <th>capital_run_length_total:</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>word_freq_make:</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.016759</td>\n",
       "      <td>0.065627</td>\n",
       "      <td>0.013273</td>\n",
       "      <td>0.023119</td>\n",
       "      <td>0.059674</td>\n",
       "      <td>0.007669</td>\n",
       "      <td>-0.003950</td>\n",
       "      <td>0.106263</td>\n",
       "      <td>0.041198</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.026505</td>\n",
       "      <td>-0.021196</td>\n",
       "      <td>-0.033301</td>\n",
       "      <td>0.058292</td>\n",
       "      <td>0.117419</td>\n",
       "      <td>-0.008844</td>\n",
       "      <td>0.044491</td>\n",
       "      <td>0.061382</td>\n",
       "      <td>0.089165</td>\n",
       "      <td>0.126208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word_freq_address:</th>\n",
       "      <td>-0.016759</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.033526</td>\n",
       "      <td>-0.006923</td>\n",
       "      <td>-0.023760</td>\n",
       "      <td>-0.024840</td>\n",
       "      <td>0.003918</td>\n",
       "      <td>-0.016280</td>\n",
       "      <td>-0.003826</td>\n",
       "      <td>0.032962</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007282</td>\n",
       "      <td>-0.049837</td>\n",
       "      <td>-0.018527</td>\n",
       "      <td>-0.014461</td>\n",
       "      <td>-0.009605</td>\n",
       "      <td>0.001946</td>\n",
       "      <td>0.002083</td>\n",
       "      <td>0.000271</td>\n",
       "      <td>-0.022680</td>\n",
       "      <td>-0.030224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word_freq_all:</th>\n",
       "      <td>0.065627</td>\n",
       "      <td>-0.033526</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.020246</td>\n",
       "      <td>0.077734</td>\n",
       "      <td>0.087564</td>\n",
       "      <td>0.036677</td>\n",
       "      <td>0.012003</td>\n",
       "      <td>0.093786</td>\n",
       "      <td>0.032075</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.033213</td>\n",
       "      <td>-0.016495</td>\n",
       "      <td>-0.033120</td>\n",
       "      <td>0.108140</td>\n",
       "      <td>0.087618</td>\n",
       "      <td>-0.003336</td>\n",
       "      <td>0.097398</td>\n",
       "      <td>0.107463</td>\n",
       "      <td>0.070114</td>\n",
       "      <td>0.196988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word_freq_3d:</th>\n",
       "      <td>0.013273</td>\n",
       "      <td>-0.006923</td>\n",
       "      <td>-0.020246</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.003238</td>\n",
       "      <td>-0.010014</td>\n",
       "      <td>0.019784</td>\n",
       "      <td>0.010268</td>\n",
       "      <td>-0.002454</td>\n",
       "      <td>-0.004947</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000591</td>\n",
       "      <td>-0.012370</td>\n",
       "      <td>-0.007148</td>\n",
       "      <td>-0.003138</td>\n",
       "      <td>0.010862</td>\n",
       "      <td>-0.000298</td>\n",
       "      <td>0.005260</td>\n",
       "      <td>0.022081</td>\n",
       "      <td>0.021369</td>\n",
       "      <td>0.057371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word_freq_our:</th>\n",
       "      <td>0.023119</td>\n",
       "      <td>-0.023760</td>\n",
       "      <td>0.077734</td>\n",
       "      <td>0.003238</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.054054</td>\n",
       "      <td>0.147336</td>\n",
       "      <td>0.029598</td>\n",
       "      <td>0.020823</td>\n",
       "      <td>0.034495</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.032759</td>\n",
       "      <td>-0.046361</td>\n",
       "      <td>-0.026390</td>\n",
       "      <td>0.025509</td>\n",
       "      <td>0.041582</td>\n",
       "      <td>0.002016</td>\n",
       "      <td>0.052662</td>\n",
       "      <td>0.052290</td>\n",
       "      <td>0.002492</td>\n",
       "      <td>0.241920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word_freq_over:</th>\n",
       "      <td>0.059674</td>\n",
       "      <td>-0.024840</td>\n",
       "      <td>0.087564</td>\n",
       "      <td>-0.010014</td>\n",
       "      <td>0.054054</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.061163</td>\n",
       "      <td>0.079561</td>\n",
       "      <td>0.117438</td>\n",
       "      <td>0.013897</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.019119</td>\n",
       "      <td>-0.008705</td>\n",
       "      <td>-0.015133</td>\n",
       "      <td>0.065043</td>\n",
       "      <td>0.105692</td>\n",
       "      <td>0.019894</td>\n",
       "      <td>-0.010278</td>\n",
       "      <td>0.090172</td>\n",
       "      <td>0.082089</td>\n",
       "      <td>0.232604</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows Ã— 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    word_freq_make:  word_freq_address:  word_freq_all:  \\\n",
       "word_freq_make:            1.000000           -0.016759        0.065627   \n",
       "word_freq_address:        -0.016759            1.000000       -0.033526   \n",
       "word_freq_all:             0.065627           -0.033526        1.000000   \n",
       "word_freq_3d:              0.013273           -0.006923       -0.020246   \n",
       "word_freq_our:             0.023119           -0.023760        0.077734   \n",
       "word_freq_over:            0.059674           -0.024840        0.087564   \n",
       "\n",
       "                    word_freq_3d:  word_freq_our:  word_freq_over:  \\\n",
       "word_freq_make:          0.013273        0.023119         0.059674   \n",
       "word_freq_address:      -0.006923       -0.023760        -0.024840   \n",
       "word_freq_all:          -0.020246        0.077734         0.087564   \n",
       "word_freq_3d:            1.000000        0.003238        -0.010014   \n",
       "word_freq_our:           0.003238        1.000000         0.054054   \n",
       "word_freq_over:         -0.010014        0.054054         1.000000   \n",
       "\n",
       "                    word_freq_remove:  word_freq_internet:  word_freq_order:  \\\n",
       "word_freq_make:              0.007669            -0.003950          0.106263   \n",
       "word_freq_address:           0.003918            -0.016280         -0.003826   \n",
       "word_freq_all:               0.036677             0.012003          0.093786   \n",
       "word_freq_3d:                0.019784             0.010268         -0.002454   \n",
       "word_freq_our:               0.147336             0.029598          0.020823   \n",
       "word_freq_over:              0.061163             0.079561          0.117438   \n",
       "\n",
       "                    word_freq_mail:  ...  char_freq_;:  char_freq_(:  \\\n",
       "word_freq_make:            0.041198  ...     -0.026505     -0.021196   \n",
       "word_freq_address:         0.032962  ...     -0.007282     -0.049837   \n",
       "word_freq_all:             0.032075  ...     -0.033213     -0.016495   \n",
       "word_freq_3d:             -0.004947  ...     -0.000591     -0.012370   \n",
       "word_freq_our:             0.034495  ...     -0.032759     -0.046361   \n",
       "word_freq_over:            0.013897  ...     -0.019119     -0.008705   \n",
       "\n",
       "                    char_freq_[:  char_freq_!:  char_freq_$:  char_freq_#:  \\\n",
       "word_freq_make:        -0.033301      0.058292      0.117419     -0.008844   \n",
       "word_freq_address:     -0.018527     -0.014461     -0.009605      0.001946   \n",
       "word_freq_all:         -0.033120      0.108140      0.087618     -0.003336   \n",
       "word_freq_3d:          -0.007148     -0.003138      0.010862     -0.000298   \n",
       "word_freq_our:         -0.026390      0.025509      0.041582      0.002016   \n",
       "word_freq_over:        -0.015133      0.065043      0.105692      0.019894   \n",
       "\n",
       "                    capital_run_length_average:  capital_run_length_longest:  \\\n",
       "word_freq_make:                        0.044491                     0.061382   \n",
       "word_freq_address:                     0.002083                     0.000271   \n",
       "word_freq_all:                         0.097398                     0.107463   \n",
       "word_freq_3d:                          0.005260                     0.022081   \n",
       "word_freq_our:                         0.052662                     0.052290   \n",
       "word_freq_over:                       -0.010278                     0.090172   \n",
       "\n",
       "                    capital_run_length_total:      spam  \n",
       "word_freq_make:                      0.089165  0.126208  \n",
       "word_freq_address:                  -0.022680 -0.030224  \n",
       "word_freq_all:                       0.070114  0.196988  \n",
       "word_freq_3d:                        0.021369  0.057371  \n",
       "word_freq_our:                       0.002492  0.241920  \n",
       "word_freq_over:                      0.082089  0.232604  \n",
       "\n",
       "[6 rows x 58 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.corr().head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19da6ca",
   "metadata": {},
   "source": [
    "## Question 3. Visualize the univariate distribution of each of the variables in the previous question. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f121463c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jvicm\\anaconda3\\lib\\site-packages\\seaborn\\distributions.py:2551: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='word_freq_remove:', ylabel='Density'>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(data['word_freq_remove:'],kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "51f344ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jvicm\\anaconda3\\lib\\site-packages\\seaborn\\distributions.py:2551: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='word_freq_your:', ylabel='Density'>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(data['word_freq_your:'],kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "94fce417",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jvicm\\anaconda3\\lib\\site-packages\\seaborn\\distributions.py:2551: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='word_freq_000:', ylabel='Density'>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(data['word_freq_000:'],kde=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f33912",
   "metadata": {},
   "source": [
    "#### 4. Name each of the supervised learning models that we have learned thus far that are used to predict dependent variables like \"spam\". \n",
    "#### KNN - K nearest neighbor K-NN algorithm  takes data which are the same or nearly the same and groups them into similar categories .As data comes in and it has a certain reltionship with a certain group of data it is grouped together in a category.\n",
    "#### SVMs are a commonly used supervised learning model for classification and regression. This method is successful with small data sets and works well with high-dimensional spaces.The system can quickly categorize fresh observations after being trained on a data set. It accomplishes this by using one or more hyperplanes to divide the data set into two categoriesNaÃ¯ve Bayes is a method that  allows classifiers to be constructed very easily , it also works very well with small datasets.\n",
    "####  Penalized logistic regression- The logistic model is penalized for having too many variables in penalized logistic regression. The coefficients of the less important variables are reduced to zero as a result of this. Regularization is another term for it.\n",
    "#### Logistic regression -Logistic regression is a process by which the discrete outcome of a certain input is modelled with a known input variable.Most popular logistic models are binary in  nature that means the take two values such as yes/ no.It is mainly used in classification problems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b22ee3",
   "metadata": {},
   "source": [
    "### Question 5. Describe the importance of training and test data. Why do we separate data into these subsets? \n",
    "Training data is used by algorithms to learn by means of observations , They are important as they have labels in which are put in a computer readable form for them to understand.The observations have one or more input variables and one output variable.\n",
    "Test data helps to measure the how accurate or efficient the algorithm used train the machine was.It also  ensures there is no overfitting or overtraining of the data. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077e5891",
   "metadata": {},
   "source": [
    "### Question 6. What is k-fold cross validation and what do we use it for? \n",
    "Cross-validation  resamples models of learning on a data sample that is limited\n",
    "That k-fold cross validation is a method that helps helps to estimate how the  model is performing on new data\n",
    "In the procedure we take one parameter K that describes the group numbering in which the sample of data is split into hence its name K fold cross validation. It helps to disclose data that was unseen .It is a simple method and easy to learn.K is chosen  i.e 10 and it hence becomes k =10 fold cross validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd89104",
   "metadata": {},
   "source": [
    "## Question 7. How is k-fold cross validation different from stratified k-fold cross validation?\n",
    "\n",
    "### Stratified k-fold cross-validation is  similar to k-fold cross-validation only that isntead of random sampling it uses stratas.\n",
    "### KFold divides a dataset into K folds , the data is stratified to make sure that all folds contained in the dataset has the same mix of observations with a certain label.Therefore  strtified K-fold is a better version of KFold as it deals with imbalanced distributions. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43f734d",
   "metadata": {},
   "source": [
    "### Question 8. Choose one model from question four. Split the data into training and test subsets. Build a model with the three variables in the dataset that you think will be good predictors of \"spam\".\n",
    "\n",
    "#### Describe why you chose any particular parameters for your model (e.g.- if you used KNN how did you decide to choose a specific value for k). Run the model and evaluate prediction error in two ways: \n",
    "\n",
    "#### Answer \n",
    "#### In the logistic regression model, Parameters used are  C,  Inverse of regularization strength; must be a positive float. C: float, default=1.0 Smaller values indicate better regularization, such like in support vector machines.\n",
    " #### Also in logistic regression if you make C really high the model effectively becomes a logistic regression model...\n",
    "#### Without looking at the data, it's impossible to say which k value is ideal. If training samples of related classes form clusters, a k value of 1 to 10 will give decent results. When data is randomly distributed, it's impossible to predict which k value will produce the greatest results. In such circumstances, you must conduct an empirical analysis to discover it.\n",
    "   #### Rather than focusing on finding an appropriate k value, modify the data using techniques like SVD and PCA. Then KNN or SVM can categorize data extremely effectively..\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc78c51",
   "metadata": {},
   "source": [
    "## a) Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4d001864",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up training and test data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "07bf0ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy : 0.8118012422360248\n",
      "testing accuracy : 0.8196958725561188\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'              precision    recall  f1-score   support\\n\\n           0       0.80      0.94      0.87       852\\n           1       0.87      0.62      0.73       529\\n\\n    accuracy                           0.82      1381\\n   macro avg       0.83      0.78      0.80      1381\\nweighted avg       0.83      0.82      0.81      1381\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "#Note: random_state ensures same data will be generated for example each time\n",
    "\n",
    "#Note: logistic regression in sklearn is preset to be a regularization model with C=100).\n",
    "#If you make C really high the model effectively becomes a logistic regression model...\n",
    "logreg = LogisticRegression(max_iter=10000)\n",
    "\n",
    "logreg.fit(X_train, y_train,)\n",
    "\n",
    "predict = logreg.predict(X_test)\n",
    "predict\n",
    "train_acc = logreg.score(X_train,y_train)\n",
    "test_acc = logreg.score(X_test,y_test)\n",
    "print(\"training accuracy :\", train_acc)\n",
    "\n",
    "print(\"testing accuracy :\", test_acc)\n",
    "\n",
    "confusion_matrix(predict,y_test)\n",
    "\n",
    "report = classification_report( y_test,predict)\n",
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e13924bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8122178161733465"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(logreg, X, y, cv=5,)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5bfd6a48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg accuracy: 0.8139486852664873\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold \n",
    "k = 5\n",
    "kf = KFold(n_splits=k,shuffle=True, random_state=None)\n",
    "model = LogisticRegression(solver= 'liblinear')\n",
    " \n",
    "result = cross_val_score(model , X, y, cv = kf)\n",
    " \n",
    "print(\"Avg accuracy: {}\".format(result.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cbca659c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg accuracy: 0.811991455412359\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold \n",
    "k = 5\n",
    "kf = KFold(n_splits=k,shuffle=True, random_state=1)\n",
    "model = LogisticRegression(solver= 'liblinear')\n",
    " \n",
    "result = cross_val_score(model , X, y, cv = kf)\n",
    " \n",
    "print(\"Avg accuracy: {}\".format(result.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa8b920",
   "metadata": {},
   "source": [
    "### Question 9. Choose a second model from question four. Using the same three variables in the dataset that you think will be good predictors of \"spam\". Describe why you chose any particular parameters for your model (e.g.- if you used KNN how did you decide to choose a specific value for k). Run the model and evaluate prediction error in two ways: A) On test data directly and B) using k-fold cross-validation. Did this model predict test data better than your previous model? \n",
    "\n",
    "### Yes the model support Vector machine performed better than logistic regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19aa632d",
   "metadata": {},
   "source": [
    "# Support Vector Machine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "65b9fa15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score is: 0.8436482084690554\n",
      "The auc score is: 0.8259266988265213\n",
      "[[501  37]\n",
      " [107 276]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.93      0.87       538\n",
      "           1       0.88      0.72      0.79       383\n",
      "\n",
      "    accuracy                           0.84       921\n",
      "   macro avg       0.85      0.83      0.83       921\n",
      "weighted avg       0.85      0.84      0.84       921\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.svm import SVC\n",
    "clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
    "clf.fit(X, y)\n",
    "# Train using 80% of the data.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# The random_state variable is here to guarantee that we all see the same numbers.\n",
    "# Note that we can provide the same parameters as in the decision tree, such as \n",
    "# min_samples_split or max_depth\n",
    "svclassifier = SVC(kernel='linear')\n",
    "svclassifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict based on the model we've trained\n",
    "y_pred = svclassifier.predict(X_test)\n",
    "print(f'The accuracy score is: {accuracy_score(y_test, y_pred)}')\n",
    "print(f'The auc score is: {roc_auc_score(y_test, y_pred)}')\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5070690c",
   "metadata": {},
   "source": [
    "### Question 10. Choose a third model from question four. Using the same three variables in the dataset that you think will be good predictors of \"spam\". Describe why you chose any particular parameters for your model (e.g.- if you used KNN how did you decide to choose a specific value for k). Run the model and evaluate prediction error in two ways: A) On test data directly and B) using k-fold cross-validation. Did this model predict test data better than your previous models?\n",
    " ### The model performed better than Support vector machine and Logistic regression\n",
    "\n",
    " # K-Nearest Neighbors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "31b3a48c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy : 0.8565217391304348\n",
      "testing accuracy : 0.8034744842562432\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[483, 126],\n",
       "       [ 55, 257]], dtype=int64)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "classifier = KNeighborsClassifier()\n",
    "\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "knn_predict = classifier.predict(X_test)\n",
    "knn_predict\n",
    "train_acc = classifier.score(X_train,y_train)\n",
    "test_acc = classifier.score(X_test,y_test)\n",
    "print(\"training accuracy :\", train_acc)\n",
    "\n",
    "print(\"testing accuracy :\", test_acc)\n",
    "\n",
    "confusion_matrix(knn_predict,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ae9ad611",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8159094556956049"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score \n",
    "scores = cross_val_score(classifier, X, y, cv=5)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfaeedf0",
   "metadata": {},
   "source": [
    "## using k fold the model definitely produced better results 0.81162 compared to 0.81590 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b68038f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg accuracy: 0.8161216541566351\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "kf = KFold(n_splits=k, shuffle=True,random_state=1)\n",
    "model = KNeighborsClassifier()\n",
    " \n",
    "result = cross_val_score(model , X, y, cv = kf)\n",
    " \n",
    "print(\"Avg accuracy: {}\".format(result.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a2655a",
   "metadata": {},
   "source": [
    "### Question 11. Choose a fourth model from question four. Using the same three variables in the dataset that you think will be good predictors of \"spam\". Describe why you chose any particular parameters for your model (e.g.- if you used KNN how did you decide to choose a specific value for k). Run the model and evaluate prediction error in two ways: A) On test data directly and B) using k-fold cross-validation. Did this model predict test data better than your previous models? \n",
    "                   ## NAIVE BAYES did not perform better than previous models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bfaea3bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score is: 0.7242128121606949\n",
      "The auc score is: 0.6710401156978267\n",
      "[[531   7]\n",
      " [247 136]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.99      0.81       538\n",
      "           1       0.95      0.36      0.52       383\n",
      "\n",
      "    accuracy                           0.72       921\n",
      "   macro avg       0.82      0.67      0.66       921\n",
      "weighted avg       0.79      0.72      0.69       921\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training the Model\n",
    "# We will start by splitting our data into training and test sets\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Fitting our model \n",
    "# Then, all that we have to do is initialize the Naive Bayes Classifier and fit the data. \n",
    "# For text classification problems, the Multinomial Naive Bayes Classifier is well-suited\n",
    "# \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "model = MultinomialNB().fit(X_train, y_train)\n",
    "\n",
    "# Evaluating the Model\n",
    "# Once we have put together our classifier, we can evaluate its performance in the testing set\n",
    "# \n",
    "predicted = model.predict(X_test)\n",
    "print(f'The accuracy score is: {accuracy_score(y_test, predicted)}')\n",
    "print(f'The auc score is: {roc_auc_score(y_test, predicted)}')\n",
    "print(confusion_matrix(y_test, predicted))\n",
    "print(classification_report(y_test, predicted))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bad031",
   "metadata": {},
   "source": [
    "## Question 12. Now rerun your best model from questions 8 through 11, but this time add three new variables to the model that you think will increase prediction accuracy. Did this model predict test data better than your previous models? \n",
    "\n",
    "## The model had better predictions than the previous models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6c75f42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_final2 =data[['word_freq_remove:', 'word_freq_your:', 'word_freq_000:','spam','word_freq_business:','word_freq_free:','char_freq_$:' ]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d7fa1b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#update data to set up for train test split\n",
    "X = data_final2.loc[:, data_final2.columns != 'spam']\n",
    "y = data_final2['spam']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "aabb7019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy : 0.8565217391304348\n",
      "testing accuracy : 0.8034744842562432\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[483, 126],\n",
       "       [ 55, 257]], dtype=int64)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "classifier = KNeighborsClassifier()\n",
    "\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "knn_predict = classifier.predict(X_test)\n",
    "knn_predict\n",
    "train_acc = classifier.score(X_train,y_train)\n",
    "test_acc = classifier.score(X_test,y_test)\n",
    "print(\"training accuracy :\", train_acc)\n",
    "\n",
    "print(\"testing accuracy :\", test_acc)\n",
    "\n",
    "confusion_matrix(knn_predict,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7041d566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg accuracy: 0.8761145730066563\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "kf = KFold(n_splits=k, shuffle=True,random_state= None)\n",
    "model = KNeighborsClassifier()\n",
    " \n",
    "result = cross_val_score(model , X, y, cv = kf)\n",
    " \n",
    "print(\"Avg accuracy: {}\".format(result.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272d8c5f",
   "metadata": {},
   "source": [
    "## Question 13. Rerun all your other models with this final set of six variables, evaluate prediction error, and choose a final model. Why did you select this model among all of the models that you ran?\n",
    "\n",
    "## The best model in all the models is KNN which hs the highest accuracy score of 0.8626"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "84f9bc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_final2 =data[['word_freq_remove:', 'word_freq_your:', 'word_freq_000:','spam','word_freq_business:','word_freq_free:','char_freq_$:' ]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "53d632af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#update data to set up for train test split\n",
    "X = data_final2.loc[:, data_final2.columns != 'spam']\n",
    "y = data_final2['spam']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292129c9",
   "metadata": {},
   "source": [
    "## Logistic regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ba306f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy : 0.8573369565217391\n",
      "testing accuracy : 0.8436482084690554\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'              precision    recall  f1-score   support\\n\\n           0       0.82      0.94      0.88       538\\n           1       0.89      0.71      0.79       383\\n\\n    accuracy                           0.84       921\\n   macro avg       0.86      0.82      0.83       921\\nweighted avg       0.85      0.84      0.84       921\\n'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "#Note: random_state ensures same data will be generated for example each time\n",
    "\n",
    "#Note: logistic regression in sklearn is preset to be a regularization model with C=100).\n",
    "#If you make C really high the model effectively becomes a logistic regression model...\n",
    "logreg = LogisticRegression(max_iter=10000)\n",
    "\n",
    "logreg.fit(X_train, y_train,)\n",
    "\n",
    "predict = logreg.predict(X_test)\n",
    "predict\n",
    "train_acc = logreg.score(X_train,y_train)\n",
    "test_acc = logreg.score(X_test,y_test)\n",
    "print(\"training accuracy :\", train_acc)\n",
    "\n",
    "print(\"testing accuracy :\", test_acc)\n",
    "\n",
    "confusion_matrix(predict,y_test)\n",
    "\n",
    "report = classification_report( y_test,predict)\n",
    "report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29f2d16",
   "metadata": {},
   "source": [
    "## B (using the second model svm with three additional variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "06a2f92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_final2 =data[['word_freq_remove:', 'word_freq_your:', 'word_freq_000:','spam','word_freq_business:','word_freq_free:','char_freq_$:' ]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d8f9cdfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#update data to set up for train test split\n",
    "X = data_final2.loc[:, data_final2.columns != 'spam']\n",
    "y = data_final2['spam']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5d4ee085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score is: 0.8436482084690554\n",
      "The auc score is: 0.8259266988265213\n",
      "[[501  37]\n",
      " [107 276]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.93      0.87       538\n",
      "           1       0.88      0.72      0.79       383\n",
      "\n",
      "    accuracy                           0.84       921\n",
      "   macro avg       0.85      0.83      0.83       921\n",
      "weighted avg       0.85      0.84      0.84       921\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.svm import SVC\n",
    "clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
    "clf.fit(X, y)\n",
    "# Train using 80% of the data.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# The random_state variable is here to guarantee that we all see the same numbers.\n",
    "# Note that we can provide the same parameters as in the decision tree, such as \n",
    "# min_samples_split or max_depth\n",
    "svclassifier = SVC(kernel='linear')\n",
    "svclassifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict based on the model we've trained\n",
    "y_pred = svclassifier.predict(X_test)\n",
    "print(f'The accuracy score is: {accuracy_score(y_test, y_pred)}')\n",
    "print(f'The auc score is: {roc_auc_score(y_test, y_pred)}')\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699dfa7f",
   "metadata": {},
   "source": [
    "# c using KNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4850eb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_final2 =data[['word_freq_remove:', 'word_freq_your:', 'word_freq_000:','spam','word_freq_business:','word_freq_free:','char_freq_$:' ]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0896abec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#update data to set up for train test split\n",
    "X = data_final2.loc[:, data_final2.columns != 'spam']\n",
    "y = data_final2['spam']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b3dc1d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy : 0.9010869565217391\n",
      "testing accuracy : 0.8773072747014115\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[501,  76],\n",
       "       [ 37, 307]], dtype=int64)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "classifier = KNeighborsClassifier()\n",
    "\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "knn_predict = classifier.predict(X_test)\n",
    "knn_predict\n",
    "train_acc = classifier.score(X_train,y_train)\n",
    "test_acc = classifier.score(X_test,y_test)\n",
    "print(\"training accuracy :\", train_acc)\n",
    "\n",
    "print(\"testing accuracy :\", test_acc)\n",
    "\n",
    "confusion_matrix(knn_predict,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4f73e01e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8626382004437521"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score \n",
    "scores = cross_val_score(classifier, X, y, cv=5)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6560e9",
   "metadata": {},
   "source": [
    "## Question 14. What variable that currently is not in your model, if included, would be likely to increase your final model's predictive power? For this answer try to speculate about a variable outside the variables available in the data that would improve you model. \n",
    "\tword_freq_free:-This is because most spam mail are usually trying to give incentives and gifts \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43132866",
   "metadata": {},
   "source": [
    "   ## Question 15. Lastly, you have listed each of the models that we have learned to use to predict dependent variables like spam. List each model we have focused on in class thus far that you could use to evaluate data with a continuous dependent variable.\n",
    "\n",
    "# Support vector Machines\n",
    "# Logistic regression \n",
    "# Linear regression\n",
    "# Ridge regression\n",
    "# Decision trees\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6a86f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
